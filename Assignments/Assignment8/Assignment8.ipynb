{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Math 157: Intro to Mathematical Software\n",
    "### UC San Diego, Winter 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Homework 8: due Thursday, March 4 at 8PM Pacific\n",
    "\n",
    "### Kernel: \n",
    "All computations in this notebook should use the SageMath kernel (although the only Sage features we are using are the plotting features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Problem 1: Basic Stylometry: Charles Dickens vs HG Wells (Lecture 20)\n",
    "\n",
    "The purpose of this problem is to compare the writing styles of Charles Dickens https://en.wikipedia.org/wiki/Charles_Dickens and H.G. Wells https://en.wikipedia.org/wiki/H._G._Wells \n",
    "\n",
    "In this directory there are two books: `warworlds.txt` and `twocities.txt`, and a passage from a third unknown book: `unk.txt`. The first book is *The War of the Worlds* by H.G. Wells and the second is *A Tale of Two Cities* by Charles Dickens; the books were obtained from Project Gutenberg. I have read in the texts as strings below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "with open('warworlds.txt','r') as f1:\n",
    "    war = f1.readlines()[0]\n",
    "with open('twocities.txt','r') as f2:\n",
    "    cities = f2.readlines()[0]\n",
    "with open('unk.txt','r') as f3:\n",
    "    unknown = f3.readlines()[0]\n",
    "print(war[:200])\n",
    "print()\n",
    "print(cities[:200])\n",
    "print()\n",
    "print(unknown[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1a.)\n",
    "\n",
    "Using NLTK's sentence tokenizer, split `war`, `cities`, and `unknown` into a list of sentences. Then make two plots. In the first plot, draw two overlapping histograms comparing *the distribution of the lengths of sentences in `unknown` to the lengths of sentences in `war`*. In the second plot, draw two overlapping histograms comparing *the distribution of the lengths of sentences in `unknown` to the lengths of sentences in `cities`*.\n",
    "\n",
    "(Reminder: the length of a sentence should be the *number of \"words\"* in the sentence. Your analysis should closely model the analysis we did in Lecture 20 with the Federalist Papers). For your histograms, use the following hyperparameters:\n",
    "- Use *different colors* for each text (Doesn't matter which colors specifically)\n",
    "- Use `alpha = .5` for each text\n",
    "- Use `density = True` for each text\n",
    "- Use `bins = [2*i for i in range(50)]` for each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import nltk\n",
    "#Do preliminary work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Compare unknown to war here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Compare unknown to cities here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1b.)\n",
    "\n",
    "NLTK also allows you to *tag* the words according to the part of speech they are. For instance, examine the sentence and the result of tagging below: NN is 'singular noun' and NNP is 'singular proper noun', JJ is 'adjective' etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "string = 'My name is Thomas, I love eating delicious hamburgers.'\n",
    "stringTokens = nltk.word_tokenize(string)\n",
    "stringTags = nltk.pos_tag(stringTokens)\n",
    "print(stringTags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "By mimicking the cell above, create three lists, `warTags` `citiesTags` and `unknownTags` consisting of the tagged word tokens appearing in each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print(warTags[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1c.) NLTK has a `FreqDist` function which computes the frequency that each tag is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "warDist = nltk.FreqDist([i[1] for i in warTags])\n",
    "citiesDist = nltk.FreqDist([i[1] for i in citiesTags])\n",
    "unknownDist = nltk.FreqDist([i[1] for i in unknownTags])\n",
    "warDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Using this, compute *rate dictionaries*. Namely, for each key in `warDist`, we should have \n",
    "$$\n",
    "warRates[key] = \\frac{\\text{number of times a word token was tagged key in War of the Worlds}}{\\text{total number of word tokens in War of the Worlds}}\n",
    "$$\n",
    "The dictionaries `citiesRates` and `unknownRates` should be defined analogously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "warRates['NN'] #Should result in ~.145"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "d.)\n",
    "\n",
    "For each of the parts of speech in the list\n",
    "```\n",
    "keys = ['NN','NNP','IN','DT','VBP','CC']\n",
    "```\n",
    "print out `warRates[key]/unknownRates[key]` and `citiesRates[key]/unknownRates[key]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "keys = ['NN','NNP','IN','DT','VBP','CC']\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "e.) \n",
    "\n",
    "Using your work in parts a.) thru d.), do you think the author of the unknown text was Charles Dickens or HG Wells?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If you want to learn more about NLTK's pos tagger, you can look here: https://medium.com/@muddaprince456/categorizing-and-pos-tagging-with-nltk-python-28f2bc9312c3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Problem 2: Zipf's Law and The Scarlet Letter (Lecture 20)\n",
    "\n",
    "*The Scarlet Letter* is a famous piece of American Literature, written by Nathaniel Hawthorne. The book is included in the directory of this homework, again obtained from Project Gutenberg. It has been read into a string below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "with open('scarletletter.txt','r') as _:\n",
    "    book = _.readlines()[0]\n",
    "book[:1009]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "a.)\n",
    "\n",
    "Using NLTK's word tokenizer, create a list `tokens` of the word tokens in `book`. From this, create a list `lowerWords` consisting of:\n",
    "- The *lowercase version* of the tokens which *are made solely of alphabetical English letters*\n",
    "\n",
    "Note: NLTK's word tokenizer *is not perfect*, but it is good enough for our crude analysis.\n",
    "\n",
    "I have exemplified two commands which may be useful below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print('THOMAS'.lower())\n",
    "print('THOMAS'.isalpha())\n",
    "print('?'.isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "tokens = #Your code here\n",
    "lowerWords = #Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print(lowerWords[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "b.)\n",
    "\n",
    "Using your list `lowerWords`, create a dictionary `wordCounts` with \n",
    "- keys given by the words that appear in `lowerWords`\n",
    "- values given by the *number of times* that specific word appears in `lowerWords`.\n",
    "\n",
    "For example,  `wordCounts['scarlet']` should result in 115, signifying that the word `scarlet` appears 115 times in *The Scarlet Letter*.\n",
    "\n",
    "I have given you a \"head start\" below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "wordCounts = {word:0 for word in set(lowerWords)}\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print(wordCounts['scarlet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "c.) \n",
    "\n",
    "Using your `wordCounts` dictionary, create a list `commonWords` of the 50 most common words that appear in *The Scarlet Letter*. Sort `commonWords` so that the words are in order of their frequency, i.e. `commonWords[0]` should be `'the'` since the word `'the'` is the most commonly used word in *The Scarlet Letter*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print(commonWords[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "d.) \n",
    "\n",
    "Make a list `rates` of the rates of the words in `commonWords`. Namely, `rates[i]` is the rate of the word `commonWords[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print(rates[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "e.)\n",
    "\n",
    "Let $N$ be the *number of distinct words* in your list `lowerwords`. Let $H_N$ be the *Nth harmonic number*\n",
    "$$\n",
    "H_N = \\sum_{i = 1}^N \\frac{1}{i}.\n",
    "$$\n",
    "Make and display a list plot of your list `rates`, together with the function \n",
    "$$\n",
    "f(x) = \\frac{1}{H_N}\\frac{1}{(1+x)^{.94}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The similarity between the curve and the points you have plotted is an instance of *Zipf's Law*. You may read more about it here if you are interested: https://en.wikipedia.org/wiki/Zipf%27s_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Problem 3: A (Very Very) Basic Classifer (Lecture 20)\n",
    "\n",
    "For this problem we will try to determine if a person is an NBA player or not *solely by looking at their height*. Note that this is *not a useful model*, since there are many more features that are important to study than just height. But this is just to give you your first introduction to building a classifier on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "a.) The file `basketball.csv` contains a mixture of data from Assignment 7, Exercise 4. Namely, it contains the heights of professional basketball players from the 2018 NBA season, as well as 1000 \"average\" humans. Each row represents one person; there is a column for their height, as well as a *binary* column `NBA` which equals\n",
    "- 1 if the person was taken from the NBA file\n",
    "- 0 if the person is an \"average\" person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('basketball.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I have imported the Support Vector Machine class from Scikit Learn. Train a SVC model on the entire `data` dataframe above; the explanatory variable is *height* and the *explained categorical variable* is whether or not they played in the NBA. Using your model, predict if a person who is 201 cm tall should be in the NBA or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn import svm\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "b.) \n",
    "\n",
    "For N = 180, 181, ..., 209, 210, predict if a person who is N centimeters tall should be in the NBA or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "c.) \n",
    "\n",
    "A basic SVC can only make classifications based on *linear inequalities*. What linear inequality is your SVC using to determine whether or not a person should be in the NBA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Problem 4: Introduction to Word Vectors (Lecture 21)\n",
    "\n",
    "The purpose of Problems 4 and 5 is to introduce you to word vectors at a basic level. Brief warning: downloading the word vectors to use in these problems will take *quite a bit* of your project's RAM. My recommendation is to solve Problems 1-3 and Problems 4-5 \"separately;\" namely, finish one group of problems and get the answers printed/written/displayed, then restart your kernel and work on the other group of problems. It is possible this is unnecessary, but it could help.\n",
    "\n",
    "\n",
    "If you *have not* \"downloaded\" the glove word vectors before, you should start problems 4 and 5 by uncommenting and running the following command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import gensim.downloader\n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If you *have* \"downloaded\" the glove word vectors before, you can run the following instead for a quicker load time (if you don't know whether or not the vectors have been downloaded, see if you have a \"gensim-data\" directory in your home project directory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#from gensim.models import KeyedVectors\n",
    "#path = '/home/user/gensim-data/glove-wiki-gigaword-50/glove-wiki-gigaword-50.gz'\n",
    "#glove_vectors = KeyedVectors.load_word2vec_format(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Run the following to ensure your vectors have been loaded properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "glove_vectors['potato']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "a.) What 5 words does the GLOVE model think are *most similar* to the word \"flag\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "b.)\n",
    "\n",
    "What word would the GLOVE model select to finish the analogy ATHENS is to GREECE as BAGHDAD is to __________\n",
    "\n",
    "Does the answer that GLOVE gives make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "c.) \n",
    "\n",
    "What word would the GLOVE model select to finish the analogy FISH is to SCHOOL as ANTS is to _________\n",
    "\n",
    "Does this answer make sense? (For those who may not know, a \"school\" is a term for a group of fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "d.)\n",
    "\n",
    "An *auto-antonym* is a word which means opposite things in different contexts: https://en.wikipedia.org/wiki/Auto-antonym. For instance, the word \"dust\" can mean both the act of *adding dust onto something* (think a \"dusting of snow\") and also the act of *removing dust from something* (think \"dusting the table\")\n",
    "\n",
    "Print out the 10 words that GLOVE thinks are *most similar* to the word \"sanction.\"\n",
    "\n",
    "In the markdown cell below, write out:\n",
    "- Two opposite definitions of the word \"sanction\" \n",
    "- Which definition most closely describes the way GLOVE thinks of the word \"sanction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Problem 5: Visualizing Romantic vs Dark Poetry with Word Vectors (Lectures 20, 21)\n",
    "\n",
    "In this directory there are several poems. The file `love.txt` contains a list of 16 lovey-dovey poems (think Pablo Neruda). The file `sad.txt` contains a list of 16 sad and depressing poems (think Edgar Allen Poe). Note that the poems are presented *in a single line*, as opposed to how they would normally be presented (as broken up into smaller, potentially rhyming, lines). I have read the poems into lists for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import scipy as sp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "with open('love.txt','r') as love:\n",
    "    lovePoems = love.readlines()[1:]\n",
    "with open('sad.txt','r') as sad:\n",
    "    sadPoems = sad.readlines()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "len(lovePoems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "len(sadPoems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "a.)\n",
    "Below is an empty list, `modifiedLovePoems`. \n",
    "For each poem in `lovePoems` do the following:\n",
    "- Call NLTK's word tokenizer to tokenize the poem into a list of *lower cased* word tokens (remember the .lower() function!)\n",
    "- Create a list of the word tokens from above which BOTH:\n",
    "    - Are at least 4 characters long, AND\n",
    "    - Appear in `glove_vectors.vocab`\n",
    "- Append this list of words to `modifiedLovePoems`\n",
    "\n",
    "For example: the following should equal `modifiedLovePoems[0]`:\n",
    "```\n",
    "['yours', 'lost', 'lost', 'although', 'long', 'lost', 'candle', 'noon', 'lost', 'snowflake', 'love', 'find', 'still', 'spirit', 'beautiful', 'bright', 'long', 'lost', 'light', 'lost', 'light', 'plunge', 'deep', 'senses', 'leave', 'deaf', 'blind', 'swept', 'tempest', 'your', 'love', 'taper', 'rushing', 'wind']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "modifiedLovePoems = []\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "b.)\n",
    "\n",
    "Repeat part a.) to create a list `modifiedSadPoems`, consisting of the sad poems which are modified in exactly the same way as above. The following should equal `modifiedSadPoems[0]`:\n",
    "```\n",
    "['laugh', 'world', 'laughs', 'with', 'weep', 'weep', 'alone', 'earth', 'must', 'borrow', 'mirth', 'trouble', 'enough', 'sing', 'hills', 'will', 'answer', 'sigh', 'lost', 'echoes', 'bound', 'joyful', 'sound', 'shrink', 'from', 'voicing', 'care', 'rejoice', 'will', 'seek', 'grieve', 'they', 'turn', 'they', 'want', 'full', 'measure', 'your', 'pleasure', 'they', 'need', 'your', 'glad', 'your', 'friends', 'many', 'lose', 'them', 'there', 'none', 'decline', 'your', 'wine', 'alone', 'must', 'drink', 'life', 'gall', 'feast', 'your', 'halls', 'crowded', 'fast', 'world', 'goes', 'succeed', 'give', 'helps', 'live', 'help', 'there', 'room', 'halls', 'pleasure', 'large', 'lordly', 'train', 'must', 'file', 'through', 'narrow', 'aisles', 'pain']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "modifiedSadPoems = []\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "c.) Create a list `averageLoveWords` for which `averageLoveWords[i]` consists of the *average word vector* of the words in the list `modifiedLovePoems[i]`. Similarly create a list `averageSadWords`.\n",
    "\n",
    "Here the average word vector of a list of words is what you might imagine: add up the word vectors associated to the words in the list and divide by the length of the list. So if my list was `L=['the','cat','the']`, the average word vector of `L` would be\n",
    "$$\n",
    "\\frac{v_{the}+v_{cat}+v_{the}}{3}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "d.) \n",
    "\n",
    "Create a 2D numpy array `poems` with 32 rows. For $0\\leq i \\leq 15$, the $i$th row should consist of `averageLoveWords[i]`. For $16\\leq i \\leq 31$, the $i$th row should consist of `averageSadWords[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, set `reducedPoems` equal to `reduce_to_2_dim(poems)`. This function projects your data to 2 dimensions in the \"best way\" possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def reduce_to_2_dim(M):\n",
    "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
    "    M_reduced = None\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "    svd = TruncatedSVD(n_components = 2, n_iter = n_iters, random_state = 42)\n",
    "    M_reduced = svd.fit_transform(M)\n",
    "    print(\"Done.\")\n",
    "    return M_reduced\n",
    "\n",
    "reducedPoems = #Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "e.)\n",
    "\n",
    "Make a list `l1` of the first 16 rows in `reducedPoems` and a list `l2` of the last 16 rows in `reducedPoems`. Make a list plot of `l1` and `l2` on the same graph, using different colors to distinguish the two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The plot you get should be relatively amazing to you...we have effectively *linearly separated* romantic poetry from gloomy poetry by simply taking the *average word of length at least 4*. No information about syntax, structure, grammar was necessary. Conclusion: Mark Zuckerberg probably knows how we feel before we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.2",
   "language": "sagemath",
   "metadata": {
    "cocalc": {
     "description": "Open-source mathematical software system",
     "priority": 10,
     "url": "https://www.sagemath.org/"
    }
   },
   "name": "sage-9.2",
   "resource_dir": "/ext/jupyter/kernels/sage-9.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}