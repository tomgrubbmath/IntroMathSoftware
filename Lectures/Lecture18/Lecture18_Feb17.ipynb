{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Lecture 18: Desiigner's Favorite Lecture\n",
    "\n",
    "### Please note: This lecture will be recorded and made available for viewing online. If you do not wish to be recorded, please adjust your camera settings accordingly. \n",
    "\n",
    "# Reminders/Announcements:\n",
    "- Assignment 6 due tomorrow at 8pm.\n",
    "- Final Project Group/Topic Assignments are available.\n",
    "- Quiz 2 is on February 22nd. Please see the Canvas announcement for details.\n",
    "    - Lecture on that Monday will just be cancelled. Twenty minutes will not get us very far with any topic these days..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pandas\n",
    "\n",
    "![](Panda-2.jpg)\n",
    "\n",
    "From the pandas website: \"pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
    "built on top of the Python programming language.\" The name comes from the phrase *PANel DAta*. The main purpose of pandas is to read, manipulate, store, and prepare datasets.\n",
    "\n",
    "Note! We are in the Python kernel again!\n",
    "\n",
    "For good introductions to Pandas, I recommend:\n",
    "- 10 minutes to pandas (although this takes *much longer* than 10 minutes...): https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html \n",
    "- Python for Data Analysis (a book by the main developer of pandas): https://www.oreilly.com/library/view/python-for-data/9781449323592/\n",
    "- Hands on Machine Learning with Scikit-Learn, Keras, & Tensorflow (a machine learning focused treatment): https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\n",
    "\n",
    "I will do my best to give a workable discussion today, but really there is a lot to explore with pandas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The main concepts in pandas are *Series* and *DataFrames*. These are similar to 1D and 2D arrays in NumPy. We will focus on the DataFrames. A DataFrame is like a mix between a 2D numpy array and an excel file. You can make DataFrames by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "pd.DataFrame([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "but a common way of making them is to feed in a csv or excel file. Let's explore this with the help of some old friends of mine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df = pd.read_csv('pokemon.csv', index_col = 1)   #Reads in a comma separated value file and turns it into a dataframe.\n",
    "print(type(df))\n",
    "print('**************************************************')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It formats much better if you forget the print!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note that the rows are indexed by the pokemon name; this was decided by the optional delimiter in the read command.\n",
    "\n",
    "As mentioned last week, the first thing you should do with data is some *cursory analysis*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.columns #Labels of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.index  #Labels of the rows (sometimes these will just be integers...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.head(7) #First few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.tail(10) #Last few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.sample(n = 8) #Random collection of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.sample(frac = 1/100)  #Random collection of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.info() #Summarize the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.describe()  #Summarize the DataFrame in a different way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.hist(bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What can we see from this basic summary? Quite a bit if we look! For example:\n",
    "- pokemon seem to come in \"weak, average, or strong\" forms (the \"total\" plot seems trimodal)\n",
    "- some stats are \"more spread out\" than others (compare Sp. Atk to Sp. Def)\n",
    "\n",
    "Let's look a bit deeper! Let's look for *correlations* between the data. First, let's extract the *numerical skill attributes*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "attr = ['Total', 'HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed']\n",
    "skills = df[attr]\n",
    "skills.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(skills, figsize = (15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Can we quantify this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "skills.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## A note on correlation coefficients...\n",
    "\n",
    "The above matrix is computing *standard correlation coefficients* of each pair (also called *Pearson's r value*). It computes how strong of a *linear* relationship exists between the variables. It can be helpful, but you have to be careful. See this picture from Wikipedia (citation:  By DenisBoigelot, original uploader was Imagecreator - Own work, original uploader was Imagecreator, CC0, https://commons.wikimedia.org/w/index.php?curid=15165296 )\n",
    "\n",
    "![](r.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Back to the dataframe\n",
    "\n",
    "How do you work with these data frames?\n",
    "\n",
    "A key tool is *loc* and *iloc*. The *loc* command extracts a row by its *named index*. The *iloc* command extracts a row by an *integer index* (be careful with this data set! The hashtag column is different than the row's index...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.loc['Bulbasaur']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can use multiindexing (like with numpy) to simultaneously restrict the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.loc['Bulbasaur','Speed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.loc['Bulbasaur',['HP','Speed','Generation']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If you don't want to bother writing the names, that is the perfect time to use iloc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.iloc[:8,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.iloc[:8:2,[1,2,3,7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A *very* powerful tool is to use *masking* with the loc function like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.loc[df['Attack'] > 170]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "How is this working? The inner argument gives us a Boolean condition for each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "bools = df['Attack'] > 170\n",
    "bools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Pandas then looks for the rows where the bool is *True*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.loc[bools]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can string these together, if you get the syntax right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.loc[((df['Attack'] > 170) and df['Legendary'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Instead of using the Python commands you're used too, you'll have to use \n",
    "- & for \"and\"\n",
    "- | for \"or\"\n",
    "- ~ for \"not\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.loc[((df['Attack'] > 170) & (df['Legendary']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.loc[((df['Attack'] > 170) | (df['HP'] >170))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Adding Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can also do entrywise operations, like with numpy, and we can use this to add data to our frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df['Offensive'] = df['Speed'] + df['Attack'] + df['Sp. Atk']\n",
    "df['Defensive'] = df['HP'] + df['Defense'] + df['Sp. Def']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Hmmm... this seems nice, but maybe we could rearrange it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df.iloc[:,[0,1,2,-4,-3,3,5,7,9,-2,4,6,8,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "newdf = df.iloc[:,[0,1,2,-4,-3,3,5,7,9,-2,4,6,8,-1]]\n",
    "newdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can also *change* entries in the dataframe in many ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "copy = newdf.copy() #Copy the data so we don't mash up our original dataset...\n",
    "copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The simplest method is to use loc to find the index you want to change, and then simply reassign that value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "copy.loc['Bulbasaur', 'Generation'] = 5\n",
    "copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Once you get more comfortable with the loc command, you can do pretty crazy stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "copy.loc[copy['Type 1'] == 'Grass', 'Type 1'] = 'Vegetable'\n",
    "copy.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "How did this work? The mask `copy['Type 1'] == 'Grass'` finds the pokemon with Type 1 given by Grass. The second parameter `Type 1` then gives the item we are going to change for these pokemon.\n",
    "\n",
    "If you were balancing the game, you could use this to uniformly \"buff\" or \"debuff\" certain characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "copy.loc[copy['Type 1'] == 'Fire', 'Defense'] = copy['Defense'] + 10\n",
    "copy.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Be careful! You would also want to update other columns in your dataframe if you did this...\n",
    "\n",
    "## ************* Participation Check***********************\n",
    "Take the `copy` dataframe and modify the 'Total' and 'Defensive' columns to reflect the \"buff\" for fire pokemon's defense (there are *many* ways to do this). Once you have done your transformation, call `copy.head(7)` to make sure you accomplished what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **********************************************************\n",
    "\n",
    "## Sorting\n",
    "\n",
    "Now that you have the data, what would you do to make the best pokemon team? You'd probably want to sort the skills somehow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "newdf.sort_values('Total').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Whoops! This gave me the worst pokemon..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "newdf.sort_values('Total', ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can also sort alphabetic data, and use multiple keys at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "newdf.sort_values('Type 1').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "newdf.sort_values(['Type 1','Total'],ascending = [True, False]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ok, what if you *really need a defensive fire type*, but you aren't allowed to use legendary pokemon...\n",
    "\n",
    "## *********** Participation Check ******************************************\n",
    "From `newdf` extract a dataframe `fire` consisting of the pokemon which are a *fire type* (either Type 1 equals Fire or Type 2 equals Fire) and which are *not legendary*. Sort the resulting dataframe to find the \"best defensive\" choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Finally, you can sort by *index* using `sort_index`. As always, this is not always useful, but occasionally you may want to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "newdf.sort_index(axis = 0).head() # Sort by ROW index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "newdf.sort_index(axis = 1).head() # Sort by COLUMN index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The groupby Command\n",
    "\n",
    "You can aggregate your data quite easily in pandas using the groupby command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "newdf.groupby(['Type 1']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Additionally, we can sort!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "newdf.groupby(['Type 1']).mean().sort_values('Defense',ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Additional commands are *sum* and *count*. These don't always make sense, but occasionally they're useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "newdf.groupby(['Type 1']).sum().sort_values('Defense',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "newdf.groupby(['Type 1']).count().sort_values('Defense',ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## *********** Participation Check ******************************************\n",
    "Try passing *a list* of parameters into `groupby`, instead of just the string 'Type 1'. What happens then? Which pair of types `(Type 1, Type 2)` has the worst average speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## *******************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Saving\n",
    "\n",
    "What good would all this analysis be if we couldn't save our results? Thankfully this is very easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "newdf.to_csv('ModifiedPokemon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "newdf.to_excel('ModifiedPokemon.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pandas and Regression (Time Permitting)\n",
    "\n",
    "Just like numpy arrays, you can use pandas dataframes as inputs into different models, such as a linear regression. Let's examine housing prices in California. This data is taken from the 1990 US Census, and displays data on (essentially) neighborhood level data. Data obtained from Aurelien Geron (https://github.com/ageron) who obtained it from Luis Torgo (https://www.dcc.fc.up.pt/~ltorgo/Regression/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing = pd.read_csv('housing.csv')\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Uh oh! We are missing some values in the \"total_bedrooms\" column. Let's *impute* by replacing 'null entries' with the corresponding mean. In pandas, blank entries are usually represented by `NaN`s. You can test for this by using the `isna()` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.loc[housing['total_bedrooms'].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Using our knowledge from above, we can now \"fill in the blanks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.loc[housing['total_bedrooms'].isna(), 'total_bedrooms'] = 537.870553"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Great! Let's keep going. Do you think this really represents California housing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.plot(kind = 'scatter', x = 'longitude', y = 'latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Remember that `alpha` parameter from way back long ago that seemed useless? It is *very* useful to describe *densities* of geographic data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.plot(kind = 'scatter', x = 'longitude', y = 'latitude', alpha = .1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What about those color map things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.plot(kind = 'scatter', x = 'longitude', y = 'latitude', alpha = .4, s = housing['population']/100, label = 'population',c = 'median_house_value', cmap = 'jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lets try to use this dataset to predict housing prices. But first; does it really make sense to consider the \"total rooms\" in the district? Is there a better option?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Also, do we really want total rooms *and* total bedrooms? Those are probably highly correlated...maybe we can just keep the *ratio* of rooms to bedrooms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Maybe this will be better...lets try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "ols = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "ols.fit(housing[['housing_median_age','population','median_income', 'roomsPerHouse','bedroomsPerRoom']], housing['median_house_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "ols.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "ols.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Looking good! What if we wanted to handle the categorical attribute? (Proximity to the ocean?)\n",
    "\n",
    "## Categorical Attributes and \"Pipelines\"\n",
    "\n",
    "Scikit learn offers many preprocessing tools which can be used to form *pipelines* on your dataset. This is essentially automated cleaning and processing of your data; for instance, when we *imputed* missing values above, that was a common \"pipeline\" technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "proximity = housing[['ocean_proximity']]\n",
    "proximity.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As with the \"Boy/Girl\" scenario in Lecture 17, we want to translate this to numeric data somehow. Scikit Learn has several builtin encoders for categorical data. The first method simply labels the data with an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordEncode = OrdinalEncoder()\n",
    "ordEncode.fit_transform(proximity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "ordEncode.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "pOrd = proximity.copy()\n",
    "pOrd['Ord'] = ordEncode.fit_transform(proximity)\n",
    "pOrd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "pOrd.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This *can be fine* in some cases, such as in a survey: if you had the choices very bad, bad, average, good, very good, then you could reasonably relabel this as 0,1,2,3,4. In this case it does not make as much sense, so we can instead use a \"one hot\" encoding. This creates *five new binary (0/1) variables*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "hotEncode = OneHotEncoder()\n",
    "Encoded = pd.DataFrame(hotEncode.fit_transform(proximity).toarray())\n",
    "Encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "proximity.join(Encoded).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You could then pipe this in to your model to get more explanation for housing prices, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training Sets, Test Sets, and Model Evaluation\n",
    "\n",
    "In practice, a *very* important preprocessing step is to split up your data into a *training set* and a *testing set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainingData, testingData = train_test_split(housing, test_size = .2, random_state = 3141592653)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "trainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "testingData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Once you do this, in practice you *completely forget about the test set* until you are done building your \"model\". Then the test set is used to, well, TEST your model. This is done in part to help prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "ols.fit(trainingData[['housing_median_age','median_income']], trainingData['median_house_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "predictions = ols.predict(testingData[['housing_median_age','median_income']])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(predictions, testingData['median_house_value'])**(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This is a *very large error*, signifying that the linear model is *underfitting* the data. Even if we add more explanatory variables, we still get a quite poor error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "ols.fit(trainingData[['housing_median_age','median_income', 'total_bedrooms']], trainingData['median_house_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "predictions = ols.predict(testingData[['housing_median_age','median_income','total_bedrooms']])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(predictions, testingData['median_house_value'])**(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This is simply because linear models are too crude of a model for this data. Perhaps if we have extra time at the end of the quarter we will discuss more powerful techniques for studying this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I think this is all for today. Please note; you could spend *months* practicing building your pipelines and working with pandas. There is much more we didn't get to here, such as \n",
    "- How to construct dataframes \"from scratch\" in Python\n",
    "- SQL style table operations\n",
    "- Pivot tables\n",
    "- etc..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}