{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Lecture 19: Pipelines and Hypothesis Tests\n",
    "\n",
    "### Please note: This lecture will be recorded and made available for viewing online. If you do not wish to be recorded, please adjust your camera settings accordingly. \n",
    "\n",
    "# Reminders/Announcements:\n",
    "- Assignment 6 has been collected. Assignment 7 coming soon (almost done with assignments!)\n",
    "- Quiz 2 is on February 22nd. Please see the Canvas announcement for details (almost done with quizzes!).\n",
    "    - Lecture on that Monday will just be cancelled. Twenty minutes will not get us very far with any topic these days..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Today we will wrap up our basic stats overview. Note that we are still in the Python kernel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pandas and Regression \n",
    "\n",
    "Just like numpy arrays, you can use pandas dataframes as inputs into different models, such as a linear regression. Let's examine housing prices in California. This data is taken from the 1990 US Census, and displays data on (essentially) neighborhood level data. Data obtained from Aurelien Geron (https://github.com/ageron) who obtained it from Luis Torgo (https://www.dcc.fc.up.pt/~ltorgo/Regression/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing = pd.read_csv('housing.csv')\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Uh oh! We are missing some values in the \"total_bedrooms\" column. Let's *impute* by replacing 'null entries' with the corresponding *average value*. In pandas, blank entries are usually represented by `NaN`s. You can test for this by using the `isna()` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.loc[housing['total_bedrooms'].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Using our knowledge from last time, we can now \"fill in the blanks\" using masking and the loc command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.loc[housing['total_bedrooms'].isna(), 'total_bedrooms'] = 537.870553"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A different alternative would have been to simply \"drop\" (remove from the dataset) any row that had missing information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.dropna() #This doesn't do anything now...since we already took care of the NaNs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Great! Let's keep going. Do you think this really represents California housing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.plot(kind = 'scatter', x = 'longitude', y = 'latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Remember that `alpha` parameter from way back long ago that seemed useless? It is *very* useful to describe *densities* of geographic data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.plot(kind = 'scatter', x = 'longitude', y = 'latitude', alpha = .1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What about those color map things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.plot(kind = 'scatter', x = 'longitude', y = 'latitude', alpha = .4, s = housing['population']/100, label = 'population',c = 'median_house_value', cmap = 'Purples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The goal with this data is to predict housing prices on the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lets try to use this dataset to predict housing prices. But first; does it really make sense to consider the \"total rooms\" in the district? Is there a better option? Also, do we really want total rooms *and* total bedrooms? Those are probably highly correlated...maybe we can just keep the *ratio* of rooms to bedrooms:\n",
    "\n",
    "## ************ Participation Check ********************\n",
    "Add a new column `roomsPerHouse` to the housing dataframe, equal to the ratio `total_rooms/households`. Then add a new column `bedroomsPerRoom` equal to the ratio `total_bedrooms/total_rooms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ***********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ok! Let's do a basic linear regression using a few of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "ols = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "ols.fit(housing[['housing_median_age','population','median_income', 'roomsPerHouse','bedroomsPerRoom']], housing['median_house_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "ols.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "ols.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "c = list(housing.columns)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#c.remove('longitude')\n",
    "housing[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Looking good! What if we wanted to handle the categorical attribute? (Proximity to the ocean is probably a big factor in housing prices!)\n",
    "\n",
    "## Categorical Attributes and \"Pipelines\"\n",
    "\n",
    "Scikit learn offers many preprocessing tools which can be used to form *pipelines* on your dataset. This is essentially automated cleaning and processing of your data; for instance, when we *imputed* missing values above, that was a common \"pipeline\" technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "proximity = housing[['ocean_proximity']]\n",
    "proximity.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "proximity.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As with the \"Boy/Girl\" scenario in Lecture 17, we want to translate this to numeric data somehow. Scikit Learn has several builtin encoders for categorical data. The first method simply labels the data with an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordEncode = OrdinalEncoder(categories = [['ISLAND','NEAR OCEAN','NEAR BAY','<1H OCEAN', 'INLAND']])\n",
    "ordEncode.fit_transform(proximity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "pOrd = proximity.copy()\n",
    "pOrd['Ord'] = ordEncode.fit_transform(proximity)\n",
    "pOrd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "pOrd.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This *can be fine* in some cases, such as in a survey: if you had the choices very bad, bad, average, good, very good, then you could reasonably relabel this as 0,1,2,3,4. In this case, this method has *some potential*; we are measuring \"distance from the ocean\" somehow. But it is not so clear that this is the right method.\n",
    "\n",
    "An alternative is to use a *one hot encoding*, which creates *5 new binary variables*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "hotEncode = OneHotEncoder()\n",
    "Encoded = pd.DataFrame(hotEncode.fit_transform(proximity).toarray())\n",
    "Encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "proximity.join(Encoded).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You could then pipe this in to your model to get more explanation for housing prices, etc. Let's add these one-hot variables to our original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing = housing.join(Encoded)\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training Sets, Test Sets, and Model Evaluation\n",
    "\n",
    "In practice, a *very* important preprocessing step is to split up your data into a *training set* and a *testing set*. Sometimes the data will already be \"shuffled\" and you can simply take the first ~80\\% as training, and the remaining 20\\% as a test set. In other cases you will need to shuffle the data yourself. Thankfully, Scikit has a builtin for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainingData, testingData = train_test_split(housing, test_size = .2, random_state = 3141592653)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "trainingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "testingData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Once you do this, in practice you *completely forget about the test set* until you are done building your \"model\". Then the test set is used to, well, TEST your model. This is done in part to help prevent \"overfitting,\" which will be explored in the homework.\n",
    "\n",
    "## ***** Extended Participation Check *********************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Train a linear regression on the *training data* dataframe. Only use `housing_median_age` and `median_income` as the explanatory variables. Once you have done this, do the following:\n",
    "- use the model to *predict* the housing prices of the houses in the *training data set*, based on `housing_median_age` and `median_income`\n",
    "- use the imported `mean_squared_error` function to calculate the *training error* (the code for this is already written for you)\n",
    "- on *the same model*, predict the housing prices of the houses in the *testing data set* based on their `housing_median_age` and `median_income`\n",
    "- use the imported `mean_squared_error` function to calculate the *testing error*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Predict the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Compute the training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Predict the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Compute the testing error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ***********************************************************************\n",
    "\n",
    "Note: This Participation Check will be *very* relevant to a problem on the upcoming homework!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This is a *very large error*, signifying (in part) that the linear model is *underfitting* the data. Even if we add more explanatory variables, we still get a quite poor error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "explanatory = [] \n",
    "ols = LinearRegression()\n",
    "ols.fit(trainingData[explanatory], trainingData['median_house_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "trainPredictions = ols.predict(trainingData[explanatory])\n",
    "mean_squared_error(trainPredictions, trainingData['median_house_value'])**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "testPredictions = ols.predict(testingData[explanatory])\n",
    "mean_squared_error(testPredictions, testingData['median_house_value'])**(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This is simply because linear models are too crude of a model for this data. Perhaps if we have extra time at the end of the quarter we will discuss more powerful techniques for studying this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Change of Pace: Hypothesis Testing\n",
    "\n",
    "If you have a larger background in statistics, you may be concerned that I have not been mentioning things like \"null hypotheses.\" I think getting into the details of that is *a bit* out of the scope for this class, but let me at least mention it briefly. (In particular, this will be *very useful* for a problem on your upcoming homework...)\n",
    "\n",
    "Suppose you had two collections of data. For example; maybe a biologist provided you with two files containing data on the *weights* of two different species of frog (in this case I will simply be making up a hypothetical data set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "s = scipy.stats.norm()\n",
    "a = 1/2.33\n",
    "t = scipy.stats.tukeylambda(a)\n",
    "frog1 = pd.DataFrame([i+7 for i in s.rvs(size = 10)])\n",
    "frog2 = pd.DataFrame([i+6.5 for i in t.rvs(size = 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "frog1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "frog2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's start by plotting out the weights for each frog to look at their distribution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy\n",
    "bins = numpy.linspace(0, 12, 20)\n",
    "\n",
    "pyplot.hist(frog1[0], bins, alpha=0.5, label='Frog 1')\n",
    "pyplot.hist(frog2[0], bins, alpha=0.5, label='Frog 2')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Do these two species of frogs have the same distribution of weights? Hard to tell...we only sampled 10 frogs of each species! What if we just got really heavy blue frogs on accident?!\n",
    "\n",
    "The concept of a *hypothesis test* is to quantify our uncertainty in this area. A basic question you could ask is the following: is the *average weight* of species 1 equal to the *average weight* of species 2?\n",
    "\n",
    "To do this, we can do a *t-test*. WARNING: there are statistical hypotheses that need to be met in order for a t-test to be valid. I will not get into them here, but please be careful if you want to do this in the real world (see more here https://en.wikipedia.org/wiki/Student%27s_t-test)\n",
    "\n",
    "To do this with scipy, you can call `scipy.stats.ttest_ind`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "scipy.stats.ttest_ind(frog1,frog2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "How should we interpret this? An important thing to look at is the *p value*. The p value is a number between 0 and 1 which says, roughly, how confident we are that the means of the two frog weights are different. The way to interpret it is: *if* the two means were the same, then the fraction of experiments that would result in this outcome \"randomly\" would be p. \n",
    "\n",
    "So *if* the frogs had the same average weight and *if* we reran this experiment 100 times (collecting 10 frogs from each species and weighing them) then this outcome would happen 100*p times \"by chance\".\n",
    "\n",
    "A common cutoff to \"reject the null hypothesis\" is p = .05. I.e. *if* you get a p-value less than .05, then you can *reject the null hypothesis that the frogs have the same average weight*.\n",
    "\n",
    "If your p value is larger than that, then you *cannot* say that the frogs *do* have the same average weight; you simply haven't \"proven\" that they don't. Maybe you simply need to increase the size of your experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "frog1 = pd.DataFrame([i+7 for i in s.rvs(size=10000)])\n",
    "frog2 = pd.DataFrame([i+6.5 for i in t.rvs(size=10000)])\n",
    "bins = numpy.linspace(0, 12, 30)\n",
    "pyplot.hist(frog1, bins, alpha=0.5, label='Frog 1')\n",
    "pyplot.hist(frog2, bins, alpha=0.5, label='Frog 2')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "scipy.stats.ttest_ind(frog1,frog2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hypothesis Testing and Linear Regression\n",
    "\n",
    "Hypothesis tests appear all over the place, as you can imagine.\n",
    "- Do ads on YouTube increase revenue more than ads in newpapers?\n",
    "- Does tobacco use decrease life expectancy?\n",
    "- Is family income related to college admissions?\n",
    "- ...\n",
    "\n",
    "They are almost *always* employed together with linear regressions. This is what I have been skipping out on (in part because Scikit Learn suppresses hypothesis testing in their models).\n",
    "\n",
    "The idea is, my linear regression gave me a big positive coefficient on some variable. But is that coefficient *statistically significant*, or did it happen by chance? To learn more about that, I recommend a course dedicated to this, such as statistical modelling or econometrics.\n",
    "\n",
    "If you want to see more detailed linear regressions, you don't have to necessarily use R or Stata! Python's `statsmodels` may be the module for you if you want to keep things \"pythonic\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}